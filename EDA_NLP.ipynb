{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path # for accessing the operating system directories and files\n",
    "import pandas as pd\n",
    "\n",
    "import re  # For regular expressions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  # For stop word removal (requires NLTK)\n",
    "from nltk.stem import PorterStemmer  # For stemming (requires NLTK)\n",
    "from nltk.tokenize import word_tokenize  # For tokenization (requires NLTK)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NOurie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NOurie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> What directories we have inside Datasets </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive\n",
      "Biden_Climate Change tweets_sentiments\n",
      "COP\n",
      "Environmental_News_NLP_Dataset\n",
      "News_Article_Category_Dataset\n",
      "Twitter_Climate_Change_Sentiment_Dataset\n",
      "Un_organised_colelction\n"
     ]
    }
   ],
   "source": [
    "# directory you want to list\n",
    "directory_path = Path('C:/Users/NOurie/OneDrive/AA FONTYES/AI_S_6/GP Aireas/On_GIT/AirQuality/Datasets')\n",
    "\n",
    "# .iterdir() method to iterate through all items (files and directories) in the directory\n",
    "for item in directory_path.iterdir():\n",
    "    item_name = item.name\n",
    "    print(item_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inside All directories in dataset </h4>\n",
    "<h6>Commented as the list is TOOO long</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_files_in_directory(directory_path):\n",
    "#     directory = Path(directory_path)\n",
    "#     for file in directory.glob('**/*'):\n",
    "#         if file.is_file():\n",
    "#             relative_path = file.relative_to(directory)\n",
    "#             print(relative_path)\n",
    "\n",
    "# directory_path = 'C:/Users/NOurie/OneDrive/AA FONTYES/AI_S_6/GP Aireas/On_GIT/AirQuality/Datasets'\n",
    "\n",
    "# list_files_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Inside end directory </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info.txt\n",
      "news-article-categories.csv\n"
     ]
    }
   ],
   "source": [
    "# Twitter Climate Change Sentiment Dataset\n",
    "\n",
    "def list_files_in_directory(directory_path):\n",
    "    directory = Path(directory_path)\n",
    "    for file in directory.glob('**/*'):\n",
    "        if file.is_file():\n",
    "            relative_path = file.relative_to(directory)\n",
    "            print(relative_path)\n",
    "\n",
    "directory_path = 'C:/Users/NOurie/OneDrive/AA FONTYES/AI_S_6/GP Aireas/On_GIT/AirQuality/Datasets/News_Article_Category_Dataset'\n",
    "\n",
    "list_files_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Create one Pandas dataset for each directory <br>\n",
    "2- Orient towards the start with Tweets <br>\n",
    "\n",
    "3- remove punctuation with Regular expressions <br>\n",
    "4- lowercase all letters<br> \n",
    "\n",
    "5- choosing if we want to take out the stop wards or not, and if yes what are the agreed on stop words<br>\n",
    "6- tokenize the all words and implement the decision on stop wards<br>\n",
    "7- use the tokenized words to create a bag of words<br>\n",
    "\n",
    "I do not see the value of Document term Matrix in this stage yet<br>\n",
    "\n",
    "8- create bag of words by<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_article_categories = pd.read_csv('./Datasets/News_Article_Category_Dataset/news-article-categories.csv')\n",
    "df_news_article_categories_GI = pd.read_csv('./Datasets/News_Article_Category_Dataset/info.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6877, 3)\n",
      "Index(['category', 'title', 'body'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Modeling Agencies Enabled Sexual Predators For...</td>\n",
       "      <td>In October 2017, Carolyn Kramer received a dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Actor Jeff Hiller Talks “Bright Colors And Bol...</td>\n",
       "      <td>This week I talked with actor Jeff Hiller abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>New Yorker Cover Puts Trump 'In The Hole' Afte...</td>\n",
       "      <td>The New Yorker is taking on President Donald T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                              title  \\\n",
       "0  ARTS & CULTURE  Modeling Agencies Enabled Sexual Predators For...   \n",
       "1  ARTS & CULTURE  Actor Jeff Hiller Talks “Bright Colors And Bol...   \n",
       "2  ARTS & CULTURE  New Yorker Cover Puts Trump 'In The Hole' Afte...   \n",
       "\n",
       "                                                body  \n",
       "0  In October 2017, Carolyn Kramer received a dis...  \n",
       "1  This week I talked with actor Jeff Hiller abou...  \n",
       "2  The New Yorker is taking on President Donald T...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_news_article_categories.shape)\n",
    "print(df_news_article_categories.columns)\n",
    "df_news_article_categories.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Twitter_Climate_Change_Sentiment_Dataset= pd.read_csv('./Datasets/Twitter_Climate_Change_Sentiment_Dataset/twitter_sentiment_data.csv')\n",
    "df_Twitter_Climate_Change_Sentiment_Dataset_GI = pd.read_csv('./Datasets/Twitter_Climate_Change_Sentiment_Dataset/general info.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43943, 3)\n",
      "Index(['sentiment', 'message', 'tweetid'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "      <td>793124211518832641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "      <td>793124402388832256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   \n",
       "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
       "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
       "\n",
       "              tweetid  \n",
       "0  792927353886371840  \n",
       "1  793124211518832641  \n",
       "2  793124402388832256  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_Twitter_Climate_Change_Sentiment_Dataset.shape)\n",
    "print(df_Twitter_Climate_Change_Sentiment_Dataset.columns)\n",
    "df_Twitter_Climate_Change_Sentiment_Dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Biden_Climate_Change_tweets_sentiments= pd.read_csv('./Datasets/Biden_Climate_Change_tweets_sentiments/Biden_Climate_Change_tweets_sentiments.csv')\n",
    "df_Biden_Climate_Change_tweets_sentiments_GI = pd.read_csv('./Datasets/Biden_Climate_Change_tweets_sentiments/info.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5467, 9)\n",
      "Index(['Text', 'Date', 'ID', 'Username', 'Location', 'Retweets', 'Favorites',\n",
      "       'Language', 'Entities'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>ID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Language</th>\n",
       "      <th>Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VeBo1991 The Greta's of the world don't suffe...</td>\n",
       "      <td>2023-03-31 04:17:45+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>X1</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajay Banga, the former CEO of Mastercard that ...</td>\n",
       "      <td>2023-03-31 04:17:38+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>X2</td>\n",
       "      <td>Boston, Massachusetts</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well if you can t see the curruption now you w...</td>\n",
       "      <td>2023-03-31 04:12:39+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>X3</td>\n",
       "      <td>America</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  @VeBo1991 The Greta's of the world don't suffe...   \n",
       "1  Ajay Banga, the former CEO of Mastercard that ...   \n",
       "2  Well if you can t see the curruption now you w...   \n",
       "\n",
       "                        Date  ID Username               Location  Retweets  \\\n",
       "0  2023-03-31 04:17:45+00:00   1       X1             Boston, MA         0   \n",
       "1  2023-03-31 04:17:38+00:00   2       X2  Boston, Massachusetts         0   \n",
       "2  2023-03-31 04:12:39+00:00   3       X3                America         0   \n",
       "\n",
       "   Favorites Language                                           Entities  \n",
       "0          0       en  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "1          1       en  {'hashtags': [], 'symbols': [], 'user_mentions...  \n",
       "2          0       en  {'hashtags': [], 'symbols': [], 'user_mentions...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_Biden_Climate_Change_tweets_sentiments.shape)\n",
    "print(df_Biden_Climate_Change_tweets_sentiments.columns)\n",
    "df_Biden_Climate_Change_tweets_sentiments.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data_frames available for testing**\n",
    "\n",
    "<ol>\n",
    "  <li>df_news_article_categories</li>\n",
    "  <li>df_Twitter_Climate_Change_Sentiment_Dataset</li>\n",
    "  <li>df_Biden_Climate_Change_tweets_sentiments</li>\n",
    "  <li></li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6877, 3)\n",
      "Index(['category', 'title', 'body'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Modeling Agencies Enabled Sexual Predators For...</td>\n",
       "      <td>In October 2017, Carolyn Kramer received a dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Actor Jeff Hiller Talks “Bright Colors And Bol...</td>\n",
       "      <td>This week I talked with actor Jeff Hiller abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>New Yorker Cover Puts Trump 'In The Hole' Afte...</td>\n",
       "      <td>The New Yorker is taking on President Donald T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                              title  \\\n",
       "0  ARTS & CULTURE  Modeling Agencies Enabled Sexual Predators For...   \n",
       "1  ARTS & CULTURE  Actor Jeff Hiller Talks “Bright Colors And Bol...   \n",
       "2  ARTS & CULTURE  New Yorker Cover Puts Trump 'In The Hole' Afte...   \n",
       "\n",
       "                                                body  \n",
       "0  In October 2017, Carolyn Kramer received a dis...  \n",
       "1  This week I talked with actor Jeff Hiller abou...  \n",
       "2  The New Yorker is taking on President Donald T...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_news_article_categories.shape)\n",
    "print(df_news_article_categories.columns)\n",
    "df_news_article_categories.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "artice_data = df_news_article_categories.body[1]\n",
    "print(type(artice_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "data = {'text': [artice_data]}\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Really handy link for dealing with Regex**\n",
    "https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  this week i talked with actor jeff hiller abou...   \n",
      "\n",
      "                                  text_word_tokenize  \\\n",
      "0  [this, week, i, talked, with, actor, jeff, hil...   \n",
      "\n",
      "                               tokenize_alphanumeric  \\\n",
      "0  [this, week, i, talked, with, actor, jeff, hil...   \n",
      "\n",
      "                        tokenize_alphanumeric_noStop  \\\n",
      "0  [week, talked, actor, jeff, hiller, hit, broad...   \n",
      "\n",
      "                tokenize_alphanumeric_noStop_stemmed  \\\n",
      "0  [week, talk, actor, jeff, hiller, hit, broadwa...   \n",
      "\n",
      "   tokenize_alphanumeric_noStop_stemmed_Contractions  \n",
      "0  [week, talk, actor, jeff, hiller, hit, broadwa...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Text Lowercasing\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# 2. Tokenization\n",
    "df['text_word_tokenize'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "# 3. Removing Punctuation\n",
    "df['tokenize_alphanumeric'] = df['text_word_tokenize'].apply(lambda tokens: [word for word in tokens if word.isalnum()]) #The following function explain this code in different way \n",
    "# def filter_alphanumeric(tokens):\n",
    "#     # Initialize an empty list to store cleaned tokens\n",
    "#     cleaned_tokens = []\n",
    "    \n",
    "#     # Iterate through each token in the list of tokens\n",
    "#     for token in tokens:\n",
    "#         # Check if the token is alphanumeric\n",
    "        # if token.isalnum():\n",
    "#             # If it's alphanumeric, add it to the cleaned tokens list\n",
    "#             cleaned_tokens.append(token)\n",
    "    \n",
    "#     return cleaned_tokens\n",
    "\n",
    "# # Apply the filter_alphanumeric function to each row in the 'text_word_tokenize' column\n",
    "# df['cleaned_text'] = df['text_word_tokenize'].apply(filter_alphanumeric)\n",
    "\n",
    "\n",
    "# 4. Removing Special Characters and Symbols\n",
    "df['tokenize_alphanumeric'] = df['tokenize_alphanumeric'].apply(lambda tokens: [re.sub(r'[^a-zA-Z0-9]', '', word) for word in tokens])\n",
    "\n",
    "# 5. Stop Word Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokenize_alphanumeric_noStop'] = df['tokenize_alphanumeric'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "# 6. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['tokenize_alphanumeric_noStop_stemmed'] = df['tokenize_alphanumeric_noStop'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
    "\n",
    "# 7. Handling Contractions (example with a simple dictionary)\n",
    "contractions = {\"won't\": \"will not\", \"can't\": \"cannot\", \"I'm\": \"I am\"}\n",
    "df['tokenize_alphanumeric_noStop_stemmed_Contractions'] = df['tokenize_alphanumeric_noStop_stemmed'].apply(lambda tokens: [contractions.get(word, word) for word in tokens])\n",
    "\n",
    "# 9. Removing HTML Tags and URLs (example with regex)\n",
    "df['tokenize_alphanumeric_noStop_stemmed_Contractions'] = df['tokenize_alphanumeric_noStop_stemmed_Contractions'].apply(lambda tokens: [re.sub(r'<[^>]+>', '', word) for word in tokens])\n",
    "df['tokenize_alphanumeric_noStop_stemmed_Contractions'] = df['tokenize_alphanumeric_noStop_stemmed_Contractions'].apply(lambda tokens: [re.sub(r'http\\S+', '', word) for word in tokens])\n",
    "\n",
    "# 10. Handling Numbers and Digits\n",
    "# df['text'] = df['text'].apply(lambda tokens: [re.sub(r'\\d+', '', word) for word in tokens])\n",
    "\n",
    "# 11. Whitespace Removal\n",
    "df['tokenize_alphanumeric_noStop_stemmed_Contractions'] = df['tokenize_alphanumeric_noStop_stemmed_Contractions'].apply(lambda tokens: [word.strip() for word in tokens])\n",
    "\n",
    "# 12. Encoding and Decoding (if needed) - Usually not needed with Pandas\n",
    "\n",
    "# 13. Handling Missing Data - Depends on your dataset\n",
    "\n",
    "# 15. Text Length Filtering (optional)\n",
    "# df['text'] = df['text'].apply(lambda tokens: [word for word in tokens if len(word) > 2])\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['week', 'talk', 'actor', 'jeff', 'hiller', 'hit', 'broadway', 'play', 'bright', 'color', 'bold', 'pattern', 'join', 'januari', '17th', 'new', 'open', 'night', 'schedul', 'februari', '4th', 'hiller', 'nightcap', '30', 'rock', 'broadway', 'bloodi', 'bloodi', 'andrew', 'jackson', 'step', 'star', 'role', 'devastatingli', 'funni', 'hit', 'play', 'direct', 'michael', 'uri', 'torch', 'song', 'buyer', 'cellar', 'ugli', 'betti', 'written', 'drew', 'droeg', 'play', 'origin', 'star', 'hiller', 'continu', 'play', 'triumphant', 'broadway', 'run', 'februari', '25th', 'soho', 'playhous', 'nyc', 'bright', 'color', 'bold', 'pattern', 'josh', 'brennan', 'get', 'marri', 'palm', 'spring', 'love', 'saturday', 'afternoon', 'howev', 'night', 'becom', 'drunken', 'riot', 'friend', 'gerri', 'arriv', 'furiou', 'invit', 'say', 'pleas', 'refrain', 'wear', 'bright', 'color', 'bold', 'play', 'produc', 'zach', 'lak', 'associ', 'riki', 'kane', 'larim', 'featur', 'set', 'design', 'dara', 'wishingrad', 'tom', 'detrini', 'serv', 'associ', 'produc', 'bright', 'color', 'bold', 'pattern', 'origin', 'present', 'vs', 'theatr', 'lo', 'angel', 'direct', 'molli', 'prather', 'talk', 'jeff', 'thrill', 'perform', 'drew', 'droeg', 'hyster', 'one', 'man', 'show', 'spin', 'lgbtq', 'issu', 'listen', 'ask', 'see', 'lgbtq', 'commun', 'move', 'forward', 'trump', 'administr', 'hiller', 'state', 'jeff', 'hiller', 'regular', 'perform', 'ucb', 'theatr', 'new', 'york', 'citi', 'lo', 'angel', 'jeff', 'also', 'written', 'star', 'pilot', 'univers', 'cabl', 'product', 'well', 'act', 'pilot', 'nbc', 'fox', 'cb', 'step', 'role', 'gerri', 'hit', 'broadway', 'play', 'bright', 'color', 'bold', 'pattern', 'start', 'januari', '17th', 'soho', 'playhous', 'nyc', 'info', 'tix', 'listen', 'lgbt', 'leader', 'alli', 'celebr', 'podcast', 'outtak', 'download', 'podcast', 'itun']\n"
     ]
    }
   ],
   "source": [
    "len(df.tokenize_alphanumeric_noStop_stemmed_Contractions[0])\n",
    "print(df.tokenize_alphanumeric_noStop_stemmed_Contractions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
